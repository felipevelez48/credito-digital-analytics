# CrÃ©dito-digital-analytics

Repositorio reproducible (VS Code + Python) que integra: (1) limpieza y tipado de la base (Excel â†’ Parquet), (2) EDA con hallazgos accionables, (3) un modelo base de desistimiento (pipeline scikitâ€‘learn + umbral Ã³ptimo) y (4) un dashboard en Streamlit con filtros, KPIs y scoring descargable. Con este flujo mostramos cÃ³mo pasar de datos crudos a insights y a una herramienta utilizable por negocio en pocas horas, sin servicios pagos.

---

## ğŸš€ Objetivos

- Parte I (diseÃ±o de mediciÃ³n): Proponer y documentar el experimento para evaluar el cambio de envÃ­o de estados de cuenta de fÃ­sico â†’ eâ€‘mail sin afectar la recuperaciÃ³n de cartera.

- Parte II (anÃ¡lisis y modelo):

    - Identificar el cliente objetivo (socioâ€‘demo y comportamiento crÃ©dito).

    - Detectar diferencias por periodos del aÃ±o.

    - Perfilar a los aprobados.

    - Entrenar un modelo para predecir desistimiento.

    ---

## ğŸ—‚ï¸ Estructura del repositorio
```
.
â”œâ”€â”€ app/                    # Streamlit app
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Excel original
â”‚   â””â”€â”€ processed/          # Parquet limpio
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ experimento_ab_test.md
â”œâ”€â”€ models/                 # model_desist.pkl + meta
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_ingesta_limpieza.ipynb
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â””â”€â”€ 02_features_modelo.ipynb
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/            # ImÃ¡genes exportadas
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§° InstalaciÃ³n y ejecuciÃ³n (Windows / Git Bash)

```
# 1) Clonar
git clone https://github.com/felipevelez48/credito-digital-analytics.git
cd credito-digital-analytics

# 2) Entorno virtual
python -m venv venv
source venv/Scripts/activate

# 3) Dependencias
pip install -r requirements.txt

# 4) Datos
# Copiar el archivo "Base PRUEBA - ANALITICA.xlsx" a data/raw/

# 5) Jupyter (opcional para notebooks)
jupyter lab  # o code notebooks/*.ipynb desde VS Code (kernel del venv)

# 6) Dashboard Streamlit
streamlit run app/app.py
```

-**Nota:** Si VS Code no muestra el kernel del venv, seleccionar Python: Select Interpreter y apuntar a venv/Scripts/python.exe.

## ğŸ” EDA â€“ Hallazgos (resumen)

Ver notebooks/01_eda.ipynb y carpeta reports/figures para detalles y grÃ¡ficos.

**- Edad:** concentraciÃ³n en rangos 20â€“34 aÃ±os (ej. distribuciÃ³n por edad_grupo).

**- Estacionalidad:** variaciÃ³n por trimestre en volumen de solicitudes y composiciÃ³n de estados (grÃ¡fico por periodo).

**- Aprobados vs. otros:** diferencias en ingresos, endeudamiento y otras variables financieras.

(Los valores exactos quedan en el EDA; este README resume.)

---

## ğŸ¤– Modelo de desistimiento (Parte IIâ€‘4)
**Variable objetivo:** estado â†’ binario (DESISTIDA=1, resto=0)

**Pipeline**

- Preprocesamiento con ColumnTransformer:

    - NumÃ©ricas â†’ StandardScaler

    - CategÃ³ricas â†’ OneHotEncoder(handle_unknown='ignore')

- Modelos evaluados (CV=5, ROCâ€‘AUC):

    - Logistic Regression (class_weight='balanced') â†’ ~0.626 Â± 0.006

    - Random Forest (n_estimators=300) â†’ ~0.613 Â± 0.047

- Tuning ligero (GridSearch con saga): mejor combinaciÃ³n L1, C=0.1 â‰ˆ 0.627 ROCâ€‘AUC.

**RestricciÃ³n prÃ¡ctica:** equipo local con poco poder de cÃ³mputo. El tuning extenso (muchas combinaciones / modelos mÃ¡s pesados) incrementa significativamente el tiempo y uso de memoria. Se priorizÃ³ un baseline estable e interpretable.

**MÃ©tricas en holdout**

- notebooks/02_features_modelo.ipynb calcula:

    - ROCâ€‘AUC en test

    - Confusion matrix y classification report (accuracy, precision, recall, f1)

    - Umbral Ã³ptimo (max F1 de clase positiva)

- Artefactos guardados en models/:

    - model_desist.pkl (pipeline completo)

    - model_meta.json (ROCâ€‘AUC, umbral)

**Criterios de selecciÃ³n**

- 1. ROCâ€‘AUC como mÃ©trica primaria por balanceâ€‘desbalance de clases.

- 2. Interpretabilidad y costo computacional (LogReg sobre RandomForest por estabilidad / CPU / RAM en este caso).

- 3. Tiempo de inferencia bajo y facilidad de despliegue.

## ğŸ“Š Dashboard (Streamlit)

- **Filtros:** aÃ±o, zona, gÃ©nero, etc.

- **KPIs:** registros filtrados, prob. media de desistir, conteo de marcados segÃºn umbral.

- **Tabla:** top solicitudes por probabilidad de desistir + descarga CSV.

- **Umbral ajustable** desde la barra lateral, usando el valor Ã³ptimo calculado.

**Ejecutar**
```
streamlit run app/app.py
```
Opciones de despliegue gratis:

- Streamlit Community Cloud (demo pÃºblica rÃ¡pida)

- Servidor propio / contenedor Docker en infraestructura interna.

---

## ğŸ§ª Parte I â€“ DiseÃ±o de mediciÃ³n (fÃ­sico vs. eâ€‘mail)

**HipÃ³tesis del jefe de cartera:** Enviar por eâ€‘mail bajarÃ¡ la recuperaciÃ³n de cartera.

**DiseÃ±o propuesto (A/B):** ver documento: docs/experimento_ab_test.md.

- AsignaciÃ³n aleatoria 50/50 (fÃ­sico vs. eâ€‘mail), estratificada por segmento (edad, zona, riesgo).

- MÃ©trica primaria: tasa de pago puntual a 30 dÃ­as; secundarias: dÃ­as de mora, % pagos totales.

- Horizonte mÃ­nimo: calcular tamaÃ±o de muestra para detectar Î”=2 pp con poder 0.8 y Î±=0.05.

- Seguimiento: corte semanal, anÃ¡lisis por intenciÃ³n de tratar (ITT), chequeo de balance.

---

## ğŸ§  Aprendizajes clave

**- Modelado local â‰  ilimitado:** con 8 GB RAM, el tuning masivo no escala; mejor baseline limpio + mejoras incrementales.

**- LogÃ­stica â‰ˆ buen primer modelo:** interpretable, rÃ¡pido y con performance decente.

**- Streamlit** es fundamental para socializar resultados y probar escenarios (whatâ€‘if); su adopciÃ³n interna es simple.

**- Exportables claros** (figuras, parquet, pkl) aceleran la presentaciÃ³n y el traspaso a equipos de ingenierÃ­a.

---

## ğŸ”­ Roadmap / PrÃ³ximos pasos

- 1. Feature engineering adicional (ej.: ratios de esfuerzo, log1p en colas, binning de variables continuas).

- 2. Modelos: HistGradientBoostingClassifier, XGBoost (CPU), calibraciÃ³n de probabilidades (Platt/Isotonic).

- 3. MLOps ligero:

    - Fijar versiones (con pip-tools o requirements.lock).

    - GitHub Actions (lint/test) en cada push.

    - script de inferencia por lotes (CLI) y endpoint simple (FastAPI opcional).

- 4. Despliegue:

    - Demo en Streamlit Cloud.

    - Docker + despliegue interno (onâ€‘prem o VM).

- 5. Parte I (operativa): instrumentar el A/B en la operaciÃ³n real con tableros de seguimiento.

---

## ğŸ“š Referencias rÃ¡pidas

- Scikitâ€‘learn: Pipelines, ColumnTransformer, LogisticRegression, RandomForest.

- Plotly + Kaleido: exportaciÃ³n de imÃ¡genes.

- Streamlit: desplegar dashboards rÃ¡pidamente.

---

# ğŸ’¡ Autor ğŸ“ŠğŸ¤–
## John Felipe VÃ©lez
### Data Engineer